{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "\n",
        "# Create folder structure\n",
        "base_dir = 'data/recycle'\n",
        "classes = ['recyclable', 'non_recyclable']\n",
        "\n",
        "for cls in classes:\n",
        "    os.makedirs(os.path.join(base_dir, cls), exist_ok=True)\n",
        "\n",
        "# Function to generate simple synthetic images\n",
        "def generate_image(cls, index):\n",
        "    img = Image.new('RGB', (64, 64), color='white')\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    if cls == 'recyclable':\n",
        "        draw.rectangle([16, 16, 48, 48], fill='green')  # green square\n",
        "    else:\n",
        "        draw.ellipse([16, 16, 48, 48], fill='red')      # red circle\n",
        "    img.save(os.path.join(base_dir, cls, f'{cls}_{index}.png'))\n",
        "\n",
        "# Generate 10 images per class\n",
        "for cls in classes:\n",
        "    for i in range(10):\n",
        "        generate_image(cls, i)\n",
        "\n",
        "print(\"Synthetic dataset created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DBDV8ox7Ib4",
        "outputId": "715f7a24-7f1b-4b85-f724-f1f258e685e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic dataset created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "train_data = train_gen.flow_from_directory(\n",
        "    'data/recycle',\n",
        "    target_size=(64, 64),\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_data = train_gen.flow_from_directory(\n",
        "    'data/recycle',\n",
        "    target_size=(64, 64),\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAxcNkK071Cn",
        "outputId": "b47b30af-b5a6-4240-8f68-de02d5492df8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 16 images belonging to 2 classes.\n",
            "Found 4 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Build a simple CNN\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(16, (3,3), activation='relu', input_shape=(64,64,3)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "history = model.fit(train_data, validation_data=val_data, epochs=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4b6ZP1U8Yro",
        "outputId": "e676fb3d-e00a-46fd-ccb7-2266f1281a77"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 0.7421 - val_accuracy: 0.5000 - val_loss: 0.5981\n",
            "Epoch 2/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.5000 - loss: 0.5981 - val_accuracy: 1.0000 - val_loss: 0.4680\n",
            "Epoch 3/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 1.0000 - loss: 0.4680 - val_accuracy: 1.0000 - val_loss: 0.3649\n",
            "Epoch 4/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step - accuracy: 1.0000 - loss: 0.3649 - val_accuracy: 1.0000 - val_loss: 0.2663\n",
            "Epoch 5/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - accuracy: 1.0000 - loss: 0.2663 - val_accuracy: 1.0000 - val_loss: 0.1831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Convert the trained model to TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open(\"recycle_model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULi8EKsr8epO",
        "outputId": "91385f0f-4901-4bb9-ba33-9b5096bf4540"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpqnk6mt0i'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  132035566748880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132035572957008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132035566747920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132035566749456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132035566749072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132035566750608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132035572956816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132035566749648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=\"recycle_model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input/output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Prepare a single test image\n",
        "img = np.array(val_data[0][0][0], dtype=np.float32).reshape(1,64,64,3)\n",
        "interpreter.set_tensor(input_details[0]['index'], img)\n",
        "interpreter.invoke()\n",
        "output = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(\"Predicted probability:\", output[0][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqFMMap28mF6",
        "outputId": "721f0f4d-a4dd-4c50-da43-d29bd6d36f04"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted probability: 0.109311804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        }
      ]
    }
  ]
}